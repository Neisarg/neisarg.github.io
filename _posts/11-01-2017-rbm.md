---
layout : post
title : Restricted Boltzmann Machines
---

<p align="justify">Probabalitic Graphical Models are a well accepted models to learn joint probability distribution from data. Marginals from this distribution can then be used to answer questions regarding data. Restricted Boltzmann Machines are the energy based models, a special case of Boltzmann Machines in which the hidden and visible layers form bipertide graph as shown in the figure below:  
</p>  
<p align="center">
<img src="/img/posts/rbm.png" style="width:450px;height:300px;">
</p>

**v** = visible variables  
**h** = hidden variables  
**w** = weight matrix  
**b** = hidden bias  
**c** = visible bias  

where energy of the model is given by :  

$$
E(\mathbf{v},\mathbf{h}) = - \mathbf{b}^T\mathbf{h} - \mathbf{c}^T\mathbf{v} - \mathbf{v}^T\mathbf{Wh}
$$  

<p align="justify">
The goal of training an RBM is to minimize the energy for a sample drawn out from the probability distribution of given input. Let's consider $\mathbf{v}$ and $\mathbf{h}$ to be binary vectors (which is the most common practice). If $b_i$ is negative, $h_i$  must be $0$ to minimize energy while it can be $1$ for positive $b_i$. Similar argument holds true for $c_j$ and $v_j$. The last term $\mathbf{v}^T\mathbf{Wh}$ can be written as $\sum_{i}\sum_{j}v_iW_{ij}h_j$. Here we can see that if $W_{ij}$ is positive, then both $v_i$ and $h_j$ must be $1$, while for a negative $W_{ij}$, at least one of $v_i$ and $h_j$ must be $0$. From this analysis we can visualize $\mathbf{b}$, $\mathbf{c}$ and $\mathbf{W}$ to be switches that force the values of visible and hidden variables to be $0$ or $1$. While training, our goal is to settle for those values of $\mathbf{b}$, $\mathbf{c}$ and $\mathbf{W}$, which force $\mathbf{v}$ to be as close as the given data.  
</p>
<p align="justify">
We can imagine the configuration of a model to be a particle with the certain energy. The probability of a particle to be in certain energy state is dependent on temperature $T$ of the particle. The joint probability of hidden and visible variables in given by:  
</p>

$$
P(\mathbf{v},\mathbf{h}) = \frac{1}{Z} e^{\frac{-E(\mathbf{v},\mathbf{h})}{T}}
$$  

<p align="justify">
Higher the temperature more stochastic is the model. At $0 T$ the model becomes deterministic Hopfield Network. Temperature can be used for guided training when initially, the temperature is kept high, so that network does not settle in local minima and explore the space. The temperature is reduced as the training proceeds to guide the training to the best minimum. This is called simulated annealing. In RBMs, we keep the temperature to be $1$ and do not change it. Hence, our equation reduces to:
</p>

$$
P(\mathbf{v},\mathbf{h}) = \frac{1}{Z} e^{-E(\mathbf{v},\mathbf{h})}
$$  

The marginal of visible variables can be computed as:

$$
P(\mathbf{v}) = \frac{1}{Z} \sum_{h} e^{-E(\mathbf{v},\mathbf{h})}
$$

and hidden variables:

$$
P(\mathbf{h}) = \frac{1}{Z} \sum_{v} e^{-E(\mathbf{v},\mathbf{h})}
$$


<p align="justify">
A simple way to understand probability and energy is that more the energy of the model for a given <b>v</b> less likely it is that the sample belongs to the probability distribution learned by model.
</p>  

$Z$ is a partition function, calculated by summing factors over all values of $\mathbf{v}$ and $\mathbf{h}$:  

$$
Z = \sum_{v}\sum_{h}e^{-E(\mathbf{v},\mathbf{h})}
$$  

<p align="justify">
One thing to note here is that the partition function is calculated by summing over all possible values of $\mathbf{v}$ and $\mathbf{h}$. This is an intractable task. To understand this, let's suppose $\mathbf{v}$ is a 10-bit binary vector. The total possible combinations of $\mathbf{v}$ are $2^{10}$ or $1024$. The corresponding $\mathbf{h}$ would also be $1024$. Hence, to compute Z we need to compute $1024\times1024$ ($1048576 !$) exponentials and add them. One might argue that modern computers are more than able to perform one million computations. But in practice $\mathbf{v}$ can be a much larger vector. For example, image size in MNIST dataset is $28\times28$ which makes an input vector of 784 which means we need to compute $2^{784}\times2^{784} !!!!$ exponentials and add them.
</p>

<p align="justify">
Restricted Boltzmann Machines are undirected graphs which satisfy Markov Property and hence fall under the category called Markov Random Fields. I will not go into the details of Markov Random Fields here, but Markov Property is very important for all practical applications of RBM. Markov Property of RBMs signifies that all visible random variables $v_i$ are independent of each other given the vector of hidden random variables $\mathbf{h}$. And similarly, all hidden random variables $h_j$ are independent of each other given the vector of visible random variables $\mathbf{v}$. Using this property let's derive the marginals of $v_i$ and $h_j$.
</p>


Joint Distribution of visible random variables given the vector of hidden variables can be written as:

$$
P(v_1, v_2, ...,v_i,..v_n | \mathbf{h} ) = \frac{ \frac{1}{Z} e^{-E({[v_1, v_2, ...,v_i,..v_n]},\mathbf{h})}}{\frac{1}{Z} \sum_{v} e^{-E(\mathbf{v},\mathbf{h})}}
$$

simplifying, we get:

$$
P(v_1, v_2, ...,v_i,..v_n | \mathbf{h} ) = \frac{ \Pi_i e^{ [c_i v_i + v_i \mathbf{W}_{i:}\mathbf{h}] } }{ \sum_{v} e^{[\mathbf{c}^T\mathbf{v} + \mathbf{v}^T\mathbf{Wh}] }}
$$

<p align="justify">
Joint Distribution of random variables can be written as the product of individual probilities.
</p>  

$$
\Pi_iP(v_i | \mathbf{h} ) = \Pi_i \frac{  e^{ [c_i v_i + v_i \mathbf{W}_{i:}\mathbf{h}] } }{ \sum_{v_i} e^{[c_i v_i + v_i \mathbf{W}_{i:}\mathbf{h}] }}
$$

Hence,  

$$
P(v_i | \mathbf{h} ) = \frac{  e^{ [c_i v_i + v_i \mathbf{W}_{i:}\mathbf{h}] } }{ \sum_{v_i} e^{[c_i v_i + v_i \mathbf{W}_{i:}\mathbf{h}] }}
$$

which gives:

$$
P(v_i =  1| \mathbf{h} ) = \frac{  e^{ [c_i  +  \mathbf{W}_{i:}\mathbf{h}] } }{ 1+ e^{[c_i  +  \mathbf{W}_{i:}\mathbf{h}] }} = \sigma(c_i  +  \mathbf{W}_{i:}\mathbf{h})
$$  

Similarly probability of $h_j$ given $\mathbf{v}$ can be written as:  

$$
P(h_j = 1 |\mathbf{v}) = \frac{  e^{ [b_j  +  \mathbf{v}\mathbf{W}_{:j}] } }{ 1+ e^{[b_j  +  \mathbf{v}\mathbf{W}_{:j}] }} = \sigma(b_j  +  \mathbf{v}\mathbf{W}_{:j})
$$  

<p align="justify">
One can see here that there is no submission over all possible values in conditional probabilities and hence they can be calculated very easily.  
</p>  

### **Error / Loss Function** ###  
<p align="justify">
The standard practice of training neural networks is to minimize the log likelihood error. Likelihood of parameters $\mathbf{b}$, $\mathbf{c}$ and $\mathbf{W}$ for the given input samples $\mathbf{x}$ can be computed as:
</p>
<p align="center">
$$
L(\mathbf{W},\mathbf{b},\mathbf{c}| \mathbf{x}) = \Pi_{t} P(\mathbf{x}^t|\mathbf{W},\mathbf{b},\mathbf{c})
$$
</p>

<p align="justify">
Since, input samples $\mathbf{x}^t$ are mounted to the visible units $\mathbf{v}$, we can denote them by $\mathbf{v}$. Also lets denote all parameters $\mathbf{b}$, $\mathbf{c}$ and $\mathbf{W}$, by $\mathbf{\theta}$. Our likelihood equation becomes:
</p>

$$
L(\mathbf{\theta}) = \Pi_{t=0}^{N-1} P(\mathbf{v}^{(t)})
$$

<p align="justify">
Log likelihood error is just the negative log likelihood and can be computed as:
</p>

$$
J(\theta) = - l(\mathbf{\theta}) = - \sum_{t=0}^{N-1} \ln(P(\mathbf{v}^{(t)}))
$$

Subsituting the marginal probability of visible variables and partition function, we get:

$$
J(\theta) = - \sum_{t=0}^{N-1}\ln{\sum_{h}e^{-E(\mathbf{v}^{(t)}, \mathbf{h})}} + N\ln{\sum_{v,h}e^{-E(\mathbf{v},\mathbf{h})}}
$$

Let's look at the derivative of loss function:

$$
\nabla_{\theta}J(\theta) = \sum_{t=0}^{N-1} \sum_{h} \frac{ e^{-E(\mathbf{v}^{(t)}, \mathbf{h})} } {\sum_{h}e^{-E(\mathbf{v}^{(t)}, \mathbf{h})}} \nabla_{\theta} E( \mathbf{v}^{(t)}, \mathbf{h})   -   N \sum_{v,h} \frac{e^{-E(\mathbf{v},\mathbf{h})}}{\sum_{v,h}e^{-E(\mathbf{v},\mathbf{h})}} \nabla_{\theta} E(\mathbf{v},\mathbf{h})
$$  

<p align="justify">
If we look carefully at the first first term in the derivative, it is nothing but the summation of expectation of $\nabla_{\theta} E(\mathbf{v},\mathbf{h})$ over distribution $P(\mathbf{h}|\mathbf{v}^{(t)})$ for all given input samples. The second term in the derivative is the expectation of $\nabla_{\theta} E(\mathbf{v},\mathbf{h})$ over joint distribution of $\mathbf{v}$ and $\mathbf{h}$ given by $P(\mathbf{v},\mathbf{h})$. Now we can rewrite the derivative of loss as:
</p>  

$$
\nabla_{\theta}J(\theta) = \sum_{t=0}^{N-1}\ \mathbb{E}_{P(\mathbf{h}|\mathbf{v}^{(t)})} [ \nabla_{\theta} E( \mathbf{v}^{(t)}, \mathbf{h})]   -   N \mathbb{E}_{P(\mathbf{v},\mathbf{h})}  [  \nabla_{\theta} E(\mathbf{v},\mathbf{h}) ]
$$  

Let's look the derivative of energy with respect to parameters:

$$
\nabla_{\mathbf{W}} E(\mathbf{v},\mathbf{h}) =  \nabla_{\mathbf{W}}(- \mathbf{b}^T\mathbf{h} - \mathbf{c}^T\mathbf{v} - \mathbf{v}^T\mathbf{Wh}) = -\mathbf{h}\mathbf{v}^T
$$  

$$
\nabla_{\mathbf{b}} E(\mathbf{v},\mathbf{h}) =  \nabla_{\mathbf{b}}(- \mathbf{b}^T\mathbf{h} - \mathbf{c}^T\mathbf{v} - \mathbf{v}^T\mathbf{Wh}) = -\mathbf{h}
$$  

$$
\nabla_{\mathbf{c}} E(\mathbf{v},\mathbf{h}) =  \nabla_{\mathbf{c}}(- \mathbf{b}^T\mathbf{h} - \mathbf{c}^T\mathbf{v} - \mathbf{v}^T\mathbf{Wh}) = -\mathbf{v}
$$

Subsituting the derivative of energy in loss derivative equation, we get:

$$
\nabla_{\mathbf{W}} J(\mathbf{W}, \mathbf{b}, \mathbf{c}) = - \sum_{t=0}^{N-1} \mathbb{E}_{P( \mathbf{h} | \mathbf{v}^{(t)} ) }[ \mathbf{h}{\mathbf{v}^{(t)}}^{T} ]  + N \mathbb{E}_{P(\mathbf{v},\mathbf{h})}  [ \mathbf{h}{\mathbf{v}^{(t)}}^{T} ]
$$  

$$
\Rightarrow \frac{\partial J}{\partial W_{ij}} = - <v_{i}h_{j}>_{data} + <v_{i}h_{j}>_{model}
$$

$$
\nabla_{\mathbf{b}} J(\mathbf{W}, \mathbf{b}, \mathbf{c}) = - \sum_{t=0}^{N-1} \mathbb{E}_{P( \mathbf{h} | \mathbf{v}^{(t)} ) }[ \mathbf{h} ]  + N \mathbb{E}_{P(\mathbf{v},\mathbf{h})}  [ \mathbf{h} ]
$$  

$$
\Rightarrow \frac{\partial J}{\partial b_{j}} = - <h_{j}>_{data} + <h_{j}>_{model}
$$

$$
\nabla_{\mathbf{c}} J(\mathbf{W}, \mathbf{b}, \mathbf{c}) = - \sum_{t=0}^{N-1} \mathbb{E}_{P( \mathbf{h} | \mathbf{v}^{(t)} ) }[ \mathbf{v}]  + N \mathbb{E}_{P(\mathbf{v},\mathbf{h})}  [ \mathbf{v} ]
$$

$$
\Rightarrow \frac{\partial J}{\partial c_{i}} = - <v_{i}>_{data} + <v_{i}>_{model}
$$  

### **Collecting Statistics with Gibbs Sampling** ###
<p align=justify>
The derivative with respect to weights has two terms, $data$, and $model$. The $data$ statistics is the configuration which we get when we show a sample to the model. While $model$ statistics is the global configuration of the model, or the configuration that model likes to be in when no input is given. Both the statistics are calculated at thermal equilibrium of model. The goal of our training is to reduce the energy of $data$ configuration and raise the energy of $model$ configuration. In other words, we want our model to learn explanation of given data samples while unlearning the explanations of all other configurations.
</p>

<p align="justify">
Instead of calculating expectation over all possible values, a good shortcut is to use point estimator. But for point estimator to be a good representation of expectation, the model needs to be in thermal equilibrium.
The $data$ statistics is collected by mounting given sample $x_{i}^{(t)}$ to $v_{i}$, and $h_{j}$ is computed with probability $P(h_{j}| \mathbf{v})$ using random number generator. Since data point is mounted to visible variables, thermal equilibrium is reached in one step itself as all hidden variables are independent given visible variables.
</p>  

<p align="justify">
This is not the case with collecting $model$ statistics. Collecting $model$ statistics is a difficult task. First, all visible and hidden variables are initialized randomly. Then visible and hidden variables are updated sequentially using conditional probabilities till network reaches thermal equilibrium, then $v_{i}$ and $h_{j}$ are sampled. This process of sampling $v_{i}$ and $h_{j}$ using conditional probabilities is called Gibbs Sampling. Gibbs Sampling is used to sample from the distributions when joint probabilities cannot be calculated but conditional probabilities are easy to calculate. The repeated sampling of all $v_{i}$ and $h_{j}$, till thermal equilibrium is called Markov Chain Monte Carlo method. Thermal equilibrium simply suggests that the probability of a particle, to be in a configuration settles to a value. This is a very slow process.
</p>

### **Contrastive Divergence** ###

<p align="justify">
While collecting $model$ statistics, we can start with data vector and we need not run full Markov Chain. We can just complete one step and get statistics. This, however, deviates from log likelihood error, but is a faster approach and practically works fine. Thus our effective weight update equation reduces to:
</p>

$$
\Delta w_{ij} = \epsilon ( <v_{i}h_{j}>^{0} - <v_{i}h_{j}>^{1} )
$$  

<p align="justify">
This works because, to know that we are going in the wrong direction we need not go till the end. Just after few steps we can get the trend and update our weights accordingly. If we consider the energy space, this technique is effectively decreasing the energy of the particle at the zeroth update and increasing the energy at the first update. This basically is making ripples in the energy field. As this is done for all the data points over and over again, these ripples merge and form surfaces with minima at given data samples. This, however, goes wrong for the possible values which are far away from data sample and have low energies. To cover up for these far away particles, we can run CD for more number of steps. The above equation is for $CD1$. We can start learning with small random weights with $CD1$ and as the weights grow, we can use $CD3$ and then $CD9$.
</p>
